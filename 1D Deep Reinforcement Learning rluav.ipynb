{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uavagent(Env):\n",
    "    def __init__(self, density):\n",
    "        # Actions we can left or right\n",
    "        self.action_space = Discrete(2) #so 0,1,2\n",
    "        # Uav allowed locations\n",
    "        self.observation_space = Box(low=np.array([0]),high=np.array([9]),dtype=int) #where is the agent\n",
    "        # Set start location\n",
    "        self.state = 0 \n",
    "        # Set start step\n",
    "        self.steps = 0\n",
    "        # Set max duration\n",
    "        self.max_steps = 80\n",
    "        # Density Functions counts \n",
    "        self.count = 10000 \n",
    "        # Copy Density\n",
    "        self.density = density\n",
    "        # Dot path\n",
    "        self.path = {}\n",
    "\n",
    "    def take_step(self,action):\n",
    "        if action ==0:\n",
    "            if self.state == 0:\n",
    "                return\n",
    "            else:\n",
    "                self.state -= 1\n",
    "        else:\n",
    "            if self.state == 9:\n",
    "                return\n",
    "            else:\n",
    "                self.state += 1\n",
    "\n",
    "    def find_reward(self, action):\n",
    "            dist = 0\n",
    "            x = self.state\n",
    "            if action == 0:\n",
    "                if x == 0:\n",
    "                    return -200\n",
    "                else:\n",
    "                    for z in self.density:\n",
    "                        dist += math.sqrt((x-z)**2)**(3/2)    \n",
    "            else:\n",
    "                if x == 9:\n",
    "                    return -200\n",
    "                else:\n",
    "                    for z in self.density:\n",
    "                        dist += math.sqrt((x-z)**2)**(3/2)\n",
    "            return -1 * (dist / self.count)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Increase steps by one\n",
    "        self.steps += 1\n",
    "        # Find reward\n",
    "        reward = self.find_reward(action)\n",
    "        # Apply Actions,  0 left, 1 right\n",
    "        self.take_step(action)\n",
    "        # Check if steps are done\n",
    "        if self.steps <= self.max_steps:\n",
    "            done = False     \n",
    "        else:\n",
    "            done = True\n",
    "        self.path[self.steps] = self.state\n",
    "        # Set place holder for info\n",
    "        info = {} #required by open gym ai ?\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self,mode=\"human\"):\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset all variables\n",
    "        self.state = 0\n",
    "        self.steps = 0\n",
    "        self.path = {}\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.214387   4.56656818 2.89157706 ... 4.36259194 3.12203713 2.400624  ]\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "mean = 4\n",
    "standard_deviation = 1\n",
    "density = np.random.normal(mean,standard_deviation,count)\n",
    "env = uavagent(density)\n",
    "print(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Episode:1 Score:-1783.3058747843295\n",
      "{1: 1, 2: 0, 3: 1, 4: 0, 5: 1, 6: 2, 7: 1, 8: 2, 9: 3, 10: 4, 11: 5, 12: 6, 13: 5, 14: 6, 15: 7, 16: 8, 17: 7, 18: 8, 19: 7, 20: 6, 21: 5, 22: 6, 23: 5, 24: 4, 25: 5, 26: 6, 27: 5, 28: 6, 29: 7, 30: 6, 31: 7, 32: 8, 33: 9, 34: 8, 35: 7, 36: 6, 37: 7, 38: 8, 39: 9, 40: 9, 41: 9, 42: 9, 43: 8, 44: 7, 45: 8, 46: 9, 47: 9, 48: 8, 49: 7, 50: 6, 51: 5, 52: 6, 53: 7, 54: 6, 55: 7, 56: 8, 57: 9, 58: 8, 59: 7, 60: 8, 61: 7, 62: 8, 63: 7, 64: 6, 65: 5, 66: 6, 67: 5, 68: 4, 69: 5, 70: 6, 71: 5, 72: 6, 73: 7, 74: 8, 75: 9, 76: 9, 77: 9, 78: 9, 79: 8, 80: 9, 81: 8}\n",
      "Episode:2 Score:-1891.743150397695\n",
      "{1: 1, 2: 0, 3: 1, 4: 2, 5: 1, 6: 0, 7: 0, 8: 0, 9: 1, 10: 0, 11: 0, 12: 1, 13: 0, 14: 1, 15: 0, 16: 1, 17: 2, 18: 3, 19: 2, 20: 1, 21: 2, 22: 3, 23: 4, 24: 3, 25: 4, 26: 3, 27: 2, 28: 1, 29: 0, 30: 1, 31: 2, 32: 3, 33: 4, 34: 5, 35: 4, 36: 3, 37: 4, 38: 3, 39: 4, 40: 3, 41: 4, 42: 3, 43: 2, 44: 1, 45: 0, 46: 1, 47: 0, 48: 0, 49: 1, 50: 2, 51: 1, 52: 0, 53: 0, 54: 1, 55: 0, 56: 1, 57: 2, 58: 1, 59: 0, 60: 0, 61: 1, 62: 0, 63: 1, 64: 0, 65: 0, 66: 0, 67: 1, 68: 2, 69: 3, 70: 4, 71: 5, 72: 4, 73: 5, 74: 4, 75: 3, 76: 2, 77: 3, 78: 4, 79: 3, 80: 4, 81: 3}\n",
      "Episode:3 Score:-1694.7522181032948\n",
      "{1: 1, 2: 0, 3: 1, 4: 2, 5: 3, 6: 2, 7: 3, 8: 2, 9: 3, 10: 4, 11: 3, 12: 2, 13: 3, 14: 4, 15: 5, 16: 4, 17: 3, 18: 4, 19: 3, 20: 4, 21: 3, 22: 2, 23: 1, 24: 0, 25: 0, 26: 0, 27: 1, 28: 0, 29: 1, 30: 0, 31: 1, 32: 2, 33: 3, 34: 4, 35: 3, 36: 2, 37: 1, 38: 0, 39: 0, 40: 0, 41: 0, 42: 1, 43: 2, 44: 3, 45: 2, 46: 1, 47: 2, 48: 3, 49: 4, 50: 5, 51: 6, 52: 7, 53: 8, 54: 9, 55: 8, 56: 7, 57: 8, 58: 9, 59: 9, 60: 9, 61: 8, 62: 7, 63: 6, 64: 5, 65: 6, 66: 7, 67: 6, 68: 5, 69: 4, 70: 3, 71: 2, 72: 1, 73: 0, 74: 1, 75: 2, 76: 1, 77: 2, 78: 3, 79: 2, 80: 1, 81: 0}\n",
      "Episode:4 Score:-211.38962542157245\n",
      "{1: 1, 2: 0, 3: 1, 4: 2, 5: 1, 6: 2, 7: 3, 8: 4, 9: 3, 10: 4, 11: 5, 12: 4, 13: 3, 14: 4, 15: 3, 16: 2, 17: 1, 18: 2, 19: 3, 20: 4, 21: 5, 22: 4, 23: 3, 24: 2, 25: 3, 26: 2, 27: 3, 28: 2, 29: 1, 30: 0, 31: 1, 32: 0, 33: 1, 34: 2, 35: 1, 36: 2, 37: 3, 38: 4, 39: 3, 40: 4, 41: 3, 42: 4, 43: 5, 44: 4, 45: 5, 46: 6, 47: 5, 48: 4, 49: 3, 50: 4, 51: 5, 52: 6, 53: 5, 54: 6, 55: 5, 56: 4, 57: 5, 58: 6, 59: 5, 60: 4, 61: 5, 62: 6, 63: 5, 64: 4, 65: 3, 66: 2, 67: 3, 68: 2, 69: 1, 70: 0, 71: 1, 72: 2, 73: 3, 74: 4, 75: 3, 76: 4, 77: 3, 78: 4, 79: 3, 80: 2, 81: 1}\n",
      "Episode:5 Score:-497.3921653166221\n",
      "{1: 1, 2: 2, 3: 1, 4: 2, 5: 1, 6: 2, 7: 3, 8: 2, 9: 3, 10: 4, 11: 3, 12: 4, 13: 5, 14: 4, 15: 3, 16: 2, 17: 1, 18: 0, 19: 1, 20: 2, 21: 1, 22: 0, 23: 0, 24: 1, 25: 2, 26: 1, 27: 2, 28: 1, 29: 2, 30: 1, 31: 2, 32: 3, 33: 2, 34: 1, 35: 2, 36: 3, 37: 4, 38: 5, 39: 6, 40: 5, 41: 6, 42: 5, 43: 6, 44: 7, 45: 6, 46: 5, 47: 4, 48: 5, 49: 4, 50: 3, 51: 2, 52: 1, 53: 2, 54: 1, 55: 0, 56: 1, 57: 0, 58: 1, 59: 0, 60: 1, 61: 2, 62: 1, 63: 0, 64: 1, 65: 2, 66: 3, 67: 4, 68: 3, 69: 2, 70: 1, 71: 2, 72: 3, 73: 4, 74: 3, 75: 2, 76: 1, 77: 2, 78: 1, 79: 0, 80: 1, 81: 2}\n",
      "Episode:6 Score:-1147.3705790516717\n",
      "{1: 1, 2: 0, 3: 1, 4: 2, 5: 1, 6: 2, 7: 3, 8: 4, 9: 5, 10: 6, 11: 7, 12: 8, 13: 7, 14: 6, 15: 5, 16: 4, 17: 5, 18: 6, 19: 7, 20: 6, 21: 5, 22: 4, 23: 5, 24: 6, 25: 5, 26: 4, 27: 5, 28: 6, 29: 7, 30: 8, 31: 7, 32: 6, 33: 7, 34: 8, 35: 9, 36: 8, 37: 9, 38: 9, 39: 8, 40: 7, 41: 8, 42: 7, 43: 6, 44: 7, 45: 8, 46: 9, 47: 9, 48: 9, 49: 8, 50: 7, 51: 8, 52: 7, 53: 8, 54: 7, 55: 6, 56: 7, 57: 8, 58: 9, 59: 9, 60: 8, 61: 9, 62: 8, 63: 7, 64: 6, 65: 5, 66: 4, 67: 5, 68: 4, 69: 5, 70: 6, 71: 5, 72: 4, 73: 5, 74: 6, 75: 5, 76: 4, 77: 3, 78: 4, 79: 3, 80: 4, 81: 5}\n",
      "Episode:7 Score:-1470.9603096488233\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 2, 6: 1, 7: 2, 8: 3, 9: 4, 10: 3, 11: 2, 12: 1, 13: 0, 14: 0, 15: 1, 16: 2, 17: 3, 18: 2, 19: 1, 20: 2, 21: 3, 22: 2, 23: 1, 24: 0, 25: 1, 26: 0, 27: 1, 28: 0, 29: 0, 30: 1, 31: 0, 32: 1, 33: 2, 34: 1, 35: 0, 36: 0, 37: 0, 38: 1, 39: 2, 40: 1, 41: 2, 42: 3, 43: 4, 44: 5, 45: 6, 46: 7, 47: 6, 48: 5, 49: 4, 50: 5, 51: 4, 52: 3, 53: 2, 54: 1, 55: 2, 56: 1, 57: 0, 58: 0, 59: 1, 60: 2, 61: 3, 62: 4, 63: 5, 64: 4, 65: 3, 66: 2, 67: 1, 68: 2, 69: 3, 70: 2, 71: 3, 72: 2, 73: 1, 74: 2, 75: 3, 76: 2, 77: 3, 78: 4, 79: 3, 80: 4, 81: 5}\n",
      "Episode:8 Score:-1427.3661415387166\n",
      "{1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: 0, 8: 1, 9: 2, 10: 3, 11: 2, 12: 1, 13: 2, 14: 1, 15: 0, 16: 1, 17: 0, 18: 1, 19: 0, 20: 0, 21: 1, 22: 2, 23: 1, 24: 0, 25: 1, 26: 2, 27: 3, 28: 4, 29: 3, 30: 2, 31: 1, 32: 2, 33: 1, 34: 0, 35: 1, 36: 2, 37: 3, 38: 2, 39: 3, 40: 2, 41: 3, 42: 4, 43: 5, 44: 6, 45: 5, 46: 6, 47: 7, 48: 8, 49: 7, 50: 8, 51: 9, 52: 8, 53: 9, 54: 9, 55: 8, 56: 7, 57: 8, 58: 7, 59: 6, 60: 7, 61: 8, 62: 7, 63: 6, 64: 5, 65: 6, 66: 7, 67: 8, 68: 9, 69: 8, 70: 9, 71: 8, 72: 9, 73: 8, 74: 9, 75: 8, 76: 9, 77: 8, 78: 7, 79: 6, 80: 7, 81: 8}\n",
      "Episode:9 Score:-2158.0484038920313\n",
      "{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 9, 11: 8, 12: 9, 13: 8, 14: 7, 15: 8, 16: 7, 17: 8, 18: 7, 19: 8, 20: 9, 21: 9, 22: 8, 23: 7, 24: 6, 25: 5, 26: 4, 27: 3, 28: 2, 29: 1, 30: 0, 31: 0, 32: 0, 33: 1, 34: 2, 35: 1, 36: 0, 37: 1, 38: 0, 39: 1, 40: 0, 41: 1, 42: 0, 43: 1, 44: 2, 45: 3, 46: 4, 47: 3, 48: 2, 49: 1, 50: 2, 51: 1, 52: 0, 53: 1, 54: 0, 55: 1, 56: 2, 57: 3, 58: 4, 59: 3, 60: 2, 61: 3, 62: 2, 63: 3, 64: 2, 65: 3, 66: 4, 67: 3, 68: 2, 69: 1, 70: 0, 71: 1, 72: 0, 73: 1, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 1, 81: 2}\n",
      "Episode:10 Score:-924.4573031153851\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    print(env.path)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 24)                48        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 698\n",
      "Trainable params: 698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From c:\\Users\\mimishal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "   10/10000 [..............................] - ETA: 3:03 - reward: -25.7624"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mimishal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 240s 24ms/step - reward: -6.6424s - rewar - ETA: 0s - r\n",
      "123 episodes - episode_reward: -539.263 [-9245.250, -194.194] - loss: 488.126 - mae: 63.262 - mean_q: -99.720\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: -2.5281\n",
      "123 episodes - episode_reward: -204.853 [-251.648, -196.456] - loss: 84.100 - mae: 60.612 - mean_q: -105.423\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: -2.4642\n",
      "124 episodes - episode_reward: -199.508 [-208.283, -196.456] - loss: 66.541 - mae: 56.628 - mean_q: -99.050\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 245s 24ms/step - reward: -2.4711s - rewar\n",
      "123 episodes - episode_reward: -200.238 [-220.110, -196.456] - loss: 66.200 - mae: 56.511 - mean_q: -99.290\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: -2.4740s - - ETA: 0s - rew\n",
      "done, took 1250.943 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b0bf265eb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: -196.456, steps: 81\n",
      "{1: 1, 2: 2, 3: 3, 4: 2, 5: 3, 6: 2, 7: 3, 8: 2, 9: 3, 10: 2, 11: 3, 12: 2, 13: 3, 14: 2, 15: 3, 16: 2, 17: 3, 18: 2, 19: 3, 20: 2, 21: 3, 22: 2, 23: 3, 24: 2, 25: 3, 26: 2, 27: 3, 28: 2, 29: 3, 30: 2, 31: 3, 32: 2, 33: 3, 34: 2, 35: 3, 36: 2, 37: 3, 38: 2, 39: 3, 40: 2, 41: 3, 42: 2, 43: 3, 44: 2, 45: 3, 46: 2, 47: 3, 48: 2, 49: 3, 50: 2, 51: 3, 52: 2, 53: 3, 54: 2, 55: 3, 56: 2, 57: 3, 58: 2, 59: 3, 60: 2, 61: 3, 62: 2, 63: 3, 64: 2, 65: 3, 66: 2, 67: 3, 68: 2, 69: 3, 70: 2, 71: 3, 72: 2, 73: 3, 74: 2, 75: 3, 76: 2, 77: 3, 78: 2, 79: 3, 80: 2, 81: 3}\n",
      "-196.45581904216175\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = dqn.test(env, nb_episodes=1,visualize=False)\n",
    "print(env.path)\n",
    "\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa16fefb63343fd10b7fdcb22ce71580f28c7cec631f473072dc978325625c0b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
